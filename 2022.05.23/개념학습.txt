Hive
- Data Warehouse
- 분산환경
- hdfs, MapReduce
- HiveQL
- 외부 시스템에서 Java를 사용하여 Hive에 접근하기
  --------------
  Java 웹사이트
- VM(Hadoop, Hive),	Host(Java)
- VM(Hadoop, Hive),	Host(Python)

인공지능
- ML(Machine Learning), 인공신경망(Deep Learging)
- Tensorflow(Keras), PyTorch

Spark
- 하둡 Eco-system
- 분석(통계, Machine Learning)
- MapReduce 대체를 위한 방법
- SQL 지원
- In-Memory : 100배 속도

Data Frame
- Spark
- pandas.DataFrame

로컬에 있는 employee.csv에 헤더를 추가(num, name, salary, job)
컬럼명이 포함된 csv 파일을 로컬에서 읽어서 화면에 표시

hdfs에서 employee.csv를 읽어와서 컬럼명을 추가하고(num, name, salary,job)
salary에 5000을 추가하여 로컬에 employee_increas.csv로 저장해보자.
저장된 파일의 내용을 다시 읽어서 화면에 표시

df=sparkSession.read.csv()
df.write.csv ('filename', mode='overwrite')

scala, python, java, R
 계정 루트 디렉토리에 pyspark_test 디렉토리 생성

DataFrame
- Spark
- pandas.DataFrame


VM(Hadoop, Spark)    <--- Host(Jupyter notebook)
 - spark-master.sh
 - spark-worker.sh 

Window에 Spark 설치, Jupyter notebook 사용

D:/VMWare_VMs/spark/스파크 압축파일 다운로드/압축해제
D:/VMWare_VMs/spark/스파크 압축해제 디렉토리/bin/winutils.exe 복사
findspark 설치 : python -m pip install findspark
환경변수 설정
SPARK_HOME = D:\VMWare_VMs\spark\spark-3.1.3-bin-hadoop3.2
HADOOP_HOME = D:\VMWare_VMs\spark\spark-3.1.3-bin-hadoop3.2
PYSPARK_DRIVER_PYTHON = jupyter
PYSPARK_DRIVER_PYTHON_OPTS = notebook
PATH = %HADOOP_HOME%\bin
JAVA_HOME = 자바가 설치된 루트 경로(bin디렉토리 상위까지)